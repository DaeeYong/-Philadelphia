{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "from dragon import dragonV\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/ivory/Documents/github/Philadelphia/real_final_data/'\n",
    "subject_list = ['pp01', 'pp02', 'pp009', 'pp085', 'pp086', 'pp087', 'pp088', 'pp089'] \n",
    "#front, rear 2가지 존재. 파일명 format : {gait_catergory}_{front || rear}.xlsx\n",
    "gait_category_list = ['gait1', 'gait2', 'fast', 'preferred', 'reaction', 'slow', 'stroop', 'turn']\n",
    "seleted_openpose_joint_idx_list = [8, 9, 10, 11, 12, 13, 14, 19, 20, 21, 22, 23, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xlsx -> nomalize -> lower joint -> frame & gt pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_data = []\n",
    "for each_subject in subject_list:\n",
    "    #root/ppxx/\n",
    "    each_subject_path = root_path + each_subject + '/'\n",
    "    files = os.listdir(each_subject_path)\n",
    "\n",
    "    ###xlsx_list###\n",
    "    excel_name_list = [file for file in files if file.endswith('.xlsx')]\n",
    "    \n",
    "    ###gt_list####\n",
    "    tmp = os.listdir(each_subject_path + 'gt/')\n",
    "    gt_name_list = [file for file in tmp if file.endswith('.npy')]\n",
    "\n",
    "    #ppxx/{each_excel_name}\n",
    "    #하나의 엑셀 파일에 대한 반복문\n",
    "    for each_excel in excel_name_list:\n",
    "        ##xlsx -> list##\n",
    "        frame_data_list = dragonV.xlsx2data(each_subject_path + each_excel)\n",
    "        #print(f'{each_excel} : {len(frame_data_list[0])}')\n",
    "        ##list -> normalize##\n",
    "        norm_frame_data_list = dragonV.nomalize_data(frame_data_list)\n",
    "        #print(f'{each_subject}->{each_excel} : {len(norm_frame_data_list[0])}')\n",
    "\n",
    "        ##noramlize -> select lower joint pos##\n",
    "        selected_norm_frame_data_list = dragonV.get_selected_joint_pos_frame_list(norm_frame_data_list, seleted_openpose_joint_idx_list)\n",
    "        #print(f'{each_subject}->{each_excel} : {len(selected_norm_frame_data_list[0])}')\n",
    "        \n",
    "        ## 여기까진 왔음...\n",
    "        ##frame data : gt pair##\n",
    "        for each_gt in gt_name_list:\n",
    "            #확장자명 제거\n",
    "            each_gt_name = each_gt[:-4]\n",
    "            if each_gt_name in each_excel:\n",
    "                #print(f'{each_subject}:{each_excel} -> {each_gt_name}')\n",
    "                right_gt_np = np.load(each_subject_path + '/gt/' + each_gt)\n",
    "                right_gt_list = right_gt_np.tolist()\n",
    "\n",
    "                data_gt_pair_list = dragonV.make_dataAndGtPair(selected_norm_frame_data_list, right_gt_list)\n",
    "                all_subject_data.append(data_gt_pair_list)\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = len(all_subject_data)\n",
    "d2 = len(all_subject_data[0])\n",
    "d3 = len(all_subject_data[0][0])\n",
    "d4_0 = len(all_subject_data[0][0][0])\n",
    "d4_1 = len(all_subject_data[0][0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_num : 73\n",
      "frame_num : 1063\n",
      "features || label : 2\n",
      "d4_0 : 26\n",
      "d4_1 : 4\n"
     ]
    }
   ],
   "source": [
    "print('video_num :',d1)\n",
    "print('frame_num :',d2)\n",
    "print('features || label :',d3)\n",
    "print('d4_0 :',d4_0)\n",
    "print('d4_1 :',d4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list -> torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98548271, 0.92060491, 0.98759102, 0.96146204, 0.9881226 ,\n",
       "       0.9740866 , 0.99484523, 0.98557527, 0.81086829, 0.91704873,\n",
       "       0.70520791, 0.97594994, 0.71989963, 0.9997064 , 0.62189694,\n",
       "       0.89097924, 0.9188043 , 0.9825343 , 0.73717281, 0.98529396,\n",
       "       0.99225403, 0.97184447, 0.98237504, 0.9717276 , 0.99244709,\n",
       "       0.97882186])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_num = 0\n",
    "frame_num = 0\n",
    "features_labels = 0\n",
    "np.array(all_subject_data[video_num][frame_num][features_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "1063\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(all_subject_data))\n",
    "print(len(all_subject_data[0]))\n",
    "print(len(all_subject_data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps이 9라서 앞, 뒤로 레이블 데이터 4개씩 버려야 함.\n",
    "'''\n",
    "each_video : frame_num x (features || label) x (26 || 4)\n",
    "'''\n",
    "trimmed_data = []\n",
    "trimmed_label = []\n",
    "\n",
    "for each_video in all_subject_data:\n",
    "    #각 비디오의 전체 프레임 길이\n",
    "    video_len = len(each_video)\n",
    "    #frame_number x each_element(26)\n",
    "    total_frame_features_list = []\n",
    "    #frame_number x each_label(4)\n",
    "    total_frame_label_list = []\n",
    "    \n",
    "    #각각의 비디오에서 frame_data와 frame별 label 값 추출\n",
    "    for i in range(0, video_len):\n",
    "        total_frame_features_list.append(each_video[i][0])\n",
    "        total_frame_label_list.append(each_video[i][1])\n",
    "\n",
    "    #timesteps 수만큼 frame data 묶기.\n",
    "    for idx in range(0, len(total_frame_features_list) - 8):\n",
    "        trimmed_data.append(total_frame_features_list[idx:idx+9])\n",
    "    \n",
    "    del total_frame_label_list[0:4]\n",
    "    del total_frame_label_list[-4:]\n",
    "    \n",
    "    for label in total_frame_label_list:\n",
    "        trimmed_label.append(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_num : 45834\n",
      "timesteps : 9\n",
      "input_size : 26\n",
      "\n",
      "\n",
      "sample_num : 45834\n",
      "label_size : 4\n"
     ]
    }
   ],
   "source": [
    "####data info###\n",
    "print(f'sample_num : {len(trimmed_data)}')\n",
    "print(f'timesteps : {len(trimmed_data[0])}')\n",
    "print(f'input_size : {len(trimmed_data[0][0])}')\n",
    "##########\n",
    "\n",
    "print('\\n')\n",
    "###label###\n",
    "print(f'sample_num : {len(trimmed_label)}')\n",
    "print(f'label_size : {len(trimmed_label[0])}')\n",
    "\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# data 저장된 곳\\ntrimmed_data : list\\ntrimmed_label : list\\n\\ntrimmed_data : sample_size x timesteps(9) x input_size(26)\\ntrimmed_label : sample_size x label_size(4)\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# data 저장된 곳\n",
    "trimmed_data : list\n",
    "trimmed_label : list\n",
    "\n",
    "trimmed_data : sample_size x timesteps(9) x input_size(26)\n",
    "trimmed_label : sample_size x label_size(4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs, _status = cell(output)<br>\n",
    "\n",
    "(모든 timesteps에 대한 결과)<br>\n",
    "outputs -> [bactch x timesteps x output]<br><br>\n",
    "(마지막 timestpes에 대한 결과)<br>\n",
    "_status -> [1 x batch x output]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_np = np.array(trimmed_data)\n",
    "label_data_np = np.array(trimmed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45834\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(trimmed_data))\n",
    "print(len(trimmed_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_tensor = torch.tensor(input_data_np, dtype=torch.float32)\n",
    "label_tensor = torch.tensor(label_data_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는지 여부: True\n"
     ]
    }
   ],
   "source": [
    "has_nan = torch.isnan(input_data_tensor).any().item()\n",
    "print(\"NaN 값이 있는지 여부:\", has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는 인덱스: tensor([[29822,     8,     0],\n",
      "        [29822,     8,     1],\n",
      "        [29822,     8,     2],\n",
      "        ...,\n",
      "        [31557,     0,    23],\n",
      "        [31557,     0,    24],\n",
      "        [31557,     0,    25]])\n"
     ]
    }
   ],
   "source": [
    "# NaN 값을 포함한 인덱스 확인\n",
    "nan_indices = torch.isnan(input_data_tensor).nonzero()\n",
    "print(\"NaN 값이 있는 인덱스:\", nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 값을 평균 값으로 대체하는 함수\n",
    "def replace_nan_with_mean(tensor):\n",
    "    # NaN 값을 평균 값으로 대체하기 위해 평균 값을 계산\n",
    "    mean_values = torch.nanmean(tensor, dim=(0, 1, 2))\n",
    "    # NaN 값을 대체할 인덱스를 찾음\n",
    "    nan_indices = torch.isnan(tensor)\n",
    "    # NaN 값을 평균 값으로 대체\n",
    "    tensor[nan_indices] = mean_values\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_tensor = replace_nan_with_mean(input_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45834, 9, 26])\n",
      "torch.Size([45834, 4])\n"
     ]
    }
   ],
   "source": [
    "print(input_data_tensor.shape)\n",
    "print(label_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 26\n",
    "output_size = 4\n",
    "hidden_size = 128\n",
    "batch_size = 10\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_data_tensor, label_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Data shape: torch.float32\n",
      "Target shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "#DataLoader 테스트\n",
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    print(\"Batch\", batch_idx)\n",
    "    print(\"Data shape:\", data.dtype)  # 미니배치의 입력 데이터 모양\n",
    "    print(\"Target shape:\", target.dtype)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## criterian && optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9, 26])\n",
      "tensor([[0.5174, 0.5334, 0.5149, 0.4919],\n",
      "        [0.5142, 0.5351, 0.5103, 0.4896],\n",
      "        [0.5050, 0.5235, 0.5131, 0.4869],\n",
      "        [0.5169, 0.5331, 0.5147, 0.4915],\n",
      "        [0.5054, 0.5219, 0.5178, 0.4874],\n",
      "        [0.5240, 0.5427, 0.5026, 0.4959],\n",
      "        [0.5180, 0.5354, 0.5153, 0.4916],\n",
      "        [0.5152, 0.5289, 0.5176, 0.4911],\n",
      "        [0.5182, 0.5332, 0.5128, 0.4934],\n",
      "        [0.5171, 0.5312, 0.5172, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1, 1, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "\n",
    "    print(data.shape)\n",
    "    outputs = model(data)\n",
    "    print(outputs)\n",
    "    print(target)\n",
    "    loss = criterion(outputs, target.float())\n",
    "    if batch_idx == 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, data_loader, num_epochs, criterion, optimizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 예측값 계산\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/1000], Loss: 0.0560, Accuracy: 279.57%\n",
      "Epoch [21/1000], Loss: 0.0480, Accuracy: 299.38%\n",
      "Epoch [31/1000], Loss: 0.0432, Accuracy: 311.30%\n",
      "Epoch [41/1000], Loss: 0.0399, Accuracy: 320.05%\n",
      "Epoch [51/1000], Loss: 0.0371, Accuracy: 326.90%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 17\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(model, data_loader, num_epochs, criterion, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(out)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_network(model, data_loader, num_epochs=num_epochs, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, threshold):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "\n",
    "    accuracy = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "\n",
    "\n",
    "    # 다중 레이블 분류 평가 지표 계산, macro : 각 클래스에 대해 개별적으로 평가\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
